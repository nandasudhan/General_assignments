{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f65710e",
   "metadata": {},
   "source": [
    "1.\n",
    "What is the main difference between logistic regression and linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524458c1",
   "metadata": {},
   "source": [
    "The main difference between logistic regression and linear regression lies in their purposes and the type of data they are designed to analyze.\n",
    "\n",
    "1.Purpose:\n",
    "\n",
    "Linear Regression: This method is used when the dependent variable (the variable being predicted) is continuous. Linear regression predicts the value of a dependent variable based on the value(s) of one or more independent variables.\n",
    "\n",
    "Logistic Regression: This technique is employed when the dependent variable is binary (e.g., yes/no, true/false, 0/1). Logistic regression models the probability that a given input belongs to a certain category.\n",
    "\n",
    "2.Output:\n",
    "\n",
    "Linear Regression: The output of linear regression is a continuous value that can range from negative to positive infinity. It predicts a numerical value.\n",
    "\n",
    "Logistic Regression: The output of logistic regression is a probability score between 0 and 1. It predicts the likelihood of an observation belonging to a certain class.\n",
    "\n",
    "3.Model Type:\n",
    "\n",
    "Linear Regression: It fits a straight line to the data points and estimates the relationship between the independent and dependent variables.\n",
    "\n",
    "Logistic Regression: It models the relationship between the independent variables and the log-odds of the dependent variable. It typically uses a logistic function to model the probability of the dependent variable.\n",
    "\n",
    "In summary, linear regression is used for predicting continuous outcomes, while logistic regression is used for predicting the probability of a binary outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c3fef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "111e0001",
   "metadata": {},
   "source": [
    "2.\n",
    "Explain the concept of regularization in logistic regression and how it helps in model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d5824",
   "metadata": {},
   "source": [
    "Regularization in logistic regression is a technique used to prevent overfitting by adding a penalty term to the cost function. The penalty term discourages the model from learning complex relationships that might fit the training data too closely, thus improving its generalization performance on unseen data.\n",
    "\n",
    "There are two common types of regularization used in logistic regression:\n",
    "\n",
    "1.L1 Regularization (Lasso): In L1 regularization, the penalty term added to the cost function is the absolute value of the coefficients of the model multiplied by a regularization parameter (λ). This regularization technique encourages sparsity in the model by driving some coefficients to exactly zero, effectively selecting a subset of the most important features.\n",
    "\n",
    "2.L2 Regularization (Ridge): In L2 regularization, the penalty term added to the cost function is the squared sum of the coefficients of the model multiplied by the regularization parameter (λ). This regularization technique shrinks the coefficients towards zero but doesn't typically force them to be exactly zero, resulting in a more stable model with potentially less variance.\n",
    "\n",
    "Regularization helps in improving model performance in several ways:\n",
    "\n",
    "a)Prevents Overfitting: By penalizing large coefficients, regularization prevents the model from fitting the noise in the training data too closely, thus reducing overfitting.\n",
    "\n",
    "b)Improves Generalization: Regularization encourages simpler models with fewer parameters, which tend to generalize better to unseen data.\n",
    "\n",
    "c)Handles Multicollinearity: Regularization can mitigate the effects of multicollinearity (high correlation between independent variables) by shrinking the coefficients towards zero, making the model more robust.\n",
    "\n",
    "d)Feature Selection: Especially with L1 regularization, some coefficients may be driven to exactly zero, effectively performing feature selection and simplifying the model.\n",
    "\n",
    "In summary, regularization in logistic regression helps in controlling model complexity, reducing overfitting, and improving the generalization performance of the model on unseen data. The choice between L1 and L2 regularization depends on the specific characteristics of the dataset and the desired properties of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0a766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98f66328",
   "metadata": {},
   "source": [
    "3.\n",
    "How does the cost function in logistic regression differ from that in linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d085499b",
   "metadata": {},
   "source": [
    "The cost function in logistic regression differs from that in linear regression primarily due to the different nature of the problems they are solving and the types of outputs they produce.\n",
    "\n",
    "1.Linear Regression Cost Function:\n",
    "\n",
    "a)The cost function used in linear regression is typically the Mean Squared Error (MSE) or the Mean Absolute Error (MAE).\n",
    "\n",
    "b)For MSE, the cost function is the sum of squared differences between the predicted values and the actual values.\n",
    "\n",
    "c)The goal of linear regression is to minimize the difference between the predicted and actual values of a continuous dependent variable.\n",
    "\n",
    "2.Logistic Regression Cost Function:\n",
    "\n",
    "a)The cost function used in logistic regression is the Cross-Entropy Loss (also known as Log Loss or Binary Cross-Entropy).\n",
    "\n",
    "b)The cross-entropy loss measures the difference between the predicted probabilities and the actual binary outcomes (0 or 1).\n",
    "\n",
    "c)Specifically, for binary logistic regression, the formula for the cross-entropy loss for one sample is:\n",
    "\n",
    "-[y log(hat(y))+ (1-y)log(1-hat(y))]\n",
    "\n",
    "where y is the actual label (0 or 1), and hat(y) is the predicted probability of the sample belonging to class 1.\n",
    "\n",
    "d)The goal of logistic regression is to maximize the likelihood of the observed data given the model parameters, which is equivalent to minimizing the cross-entropy loss.\n",
    "\n",
    "In summary, while linear regression aims to minimize the difference between predicted and actual continuous values, logistic regression aims to minimize the difference between predicted probabilities and actual binary outcomes using the cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4cc806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "897d9e17",
   "metadata": {},
   "source": [
    "4.\n",
    "What is the purpose of the sigmoid function in logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96212eab",
   "metadata": {},
   "source": [
    "The sigmoid function, also known as the logistic function, is a crucial component in logistic regression. Its purpose is to map the output of the linear combination of features and parameters to a probability value between 0 and 1. Mathematically, the sigmoid function is defined as:\n",
    "\n",
    "σ(z)= 1/(1+e^-z) \n",
    "\n",
    "where z is the linear combination of features and parameters:\n",
    "\n",
    "z = W0 + W1X1 + W2X2 +...+ WnXn\n",
    "\n",
    "In logistic regression, the sigmoid function transforms the linear combination z into a probability value hat(y), which represents the probability that a given input x belongs to the positive class (e.g., class 1 in binary classification). The output hat(y) is bounded between 0 and 1 due to the nature of the sigmoid function.\n",
    "\n",
    "The sigmoid function has several important properties that make it suitable for logistic regression:\n",
    "\n",
    "1.Output Range: The output of the sigmoid function always falls between 0 and 1, which makes it appropriate for representing probabilities.\n",
    "\n",
    "2.Smoothness: The sigmoid function is smooth and continuously differentiable, which facilitates gradient-based optimization techniques for model training, such as gradient descent.\n",
    "\n",
    "3.Monotonicity: The sigmoid function is monotonic, meaning that it strictly increases as its input increases or decreases. This property ensures that changes in the input values result in predictable changes in the output probabilities.\n",
    "\n",
    "Overall, the sigmoid function serves the critical role of transforming the linear combination of features and parameters into meaningful probability estimates in logistic regression, enabling the model to make binary classification decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7f3860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6077833",
   "metadata": {},
   "source": [
    "5.\n",
    "Explain the role of the regularization parameter in ridge regression and how it affects the model's complexity."
   ]
  },
  {
   "attachments": {
    "Screenshot%202024-02-17%20161715.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAikAAADACAYAAADBR/N9AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAAD7kSURBVHhe7d0PbBvnnSf8715eyN1caGQhoUVo6IXYBqGTg5QUq4ELM25sXoroXNhsilANHKZoWBdZKkElB6l6uUgObLFoVwliqUilBvWxdwljJFLRvrSxqYLmpdN1aNSgDs1KuMQqklB4DdNoQWEDM6+3FhDsPc/MQ3I4/DeUKHtsfz8AJc5wOPPMMw/n+c3zPEP+jcfj+XcQEREROcx/UP+JiIiIHIVBChERETkSgxQiIiJyJAYpRERE5EgMUoiIiMiRGKQQtZh3Rx98d1Y+dx43tG9EMPJsBIFet5pHROQcvAWZqIXcB2KY+BKw+cvtuLh8Gbn/b0lEKj50/K8fov9HabWUE/gw8vooev6cxtIlD7RdXmB+EuEn48iqJYiIrrYrHKR44bt/MzJvp1tyInT37oLn/z+J1AdqBjWBx0KS6e75/CY1tQb5DOZOiUBEp2E89hgWfrWK0KGtWHiqD8OngNB0EoNb5jGwdxiOCVP2x5DcdwtSR/sxekyk/McJTN2/Cann+jD0plqGiOgqu4JBig/Dr42id2kM/YdTat467RjBzMEeLBzuR1RUBtcH2QQfwANfdmM1k8TJ//ck0ufcCO73Y/lovEWV3PV6LETe3d+DWz5ZwMl5e6FX6CURQGxziWerWFk+j4ufGfNrcnVgy60utLWp6c+WMBsMYfycnBDb73Xh/O4xJHbmMOkfQFzMjcRSCLvTGO0bwpxczAn2TyH5+N3IvupD6Kdiep+YPqAh9/9o6P+RsQgR0dW2ziAlgPHjw/B9vg1tN6lZwmp+ETP+MCbVtCQrg8jfJRB8ZLKlzcnux2OIP/RXxMKiQtAriisr+GICQ9vdpf2/tIjp+8KIqUmrwOQcRra3qymRVxdSmNg7hFk50RnCxM8i0NoySJ/JiJ3rQc8dm7DwXg5a90VM65WeCAbSAXj0dzeQTxcrSrPr9ViIPcPY6yH4vyTy9yNxXB6O2gjqQpj63SC0W0V2zU8iFLHX3eHdM4zB7wWg3daGzImACPZK75JBSUjkui88LabE+pODuPujafi+V6tUXH1GSwow92QAo2fUTCKiq2ydA2cTGN7rg+8rGmKLq/qcpWMafJYABXsmEOrNI/nT1laKUvblEcz9RUP4v4XUnCtr9qmAvv+T83msXhJ5cLMX9z6uXqwQRuB2l7hmlzJIaCKvCgGKuAof/EkEPqQw3BfC0HOjGBKVoH/kXbh7vShcuANR9Iv3ad+cM/JSBiJy2vQIRCaR+CCvX/V36e8xuY6PBURgMPpwGL9dBtq+dC8e2a1m1xXHwGGRl5+J7OqNYPyAvQGkSyfGMbA3jGlx3D33iMBSzZdBSXdXG3LnkvqU+3E/7t6UwW+POjdAQWcEke0dyL4VY4BCRI7Sort7jBOzrHjP/sqYU+LG8MM+tH+UwvSGnACzGD+5iE29QYxtU7OukvPvpbEiwonurw6Lva5i/7245cM0zqvJMp1h+O5oQ/Z/z4owxeRUFM+frBJOnLuMy+qpVXY+jui341i81AH3t9RMnfOPxcjrc5gqS3OzsoidkWNE2rH1gYAxq5FToxh/MyOCxzZ4H5rAyA41v6ElxERAmLq5B8HC/m7rwRbXKlx3iXUemsLEg7cgdWQIUcdW/j6MPB9E++kJREYSah4RkTO0JkjRT8zi/0oW89ZmflH59t4hTufz4y2/ci86Ku9QcKPnG6Xr2atiZQ5nV8T/O3oR7jRmmQ3uakf27ZyastjhRof41/GFXmPaJH1iyXbeBZ4YRkivMGNIf7SKz92qzzZcC8fiJhHsmroO1yL7xgKWPhNhSvcDIny2J3V4CDN/WgXaPAgcGBNVt10JjI9MIHHRmHI/0AV3/l8QC44hceqXGOrrx/CvNiy318mHwV8M4ounRxF4ZhbZ/WMYW1eASETUWq0JUu4XJ2bxL5+ZrxwY+NBWeESVmLG2sGwLYfiQOCmaHsOPhhCpmCcru0D5/CesV8jT+PiCqCC8e6u3YFwxc3hNH7Dpgfa4pZLuHIZ20wJeO6GmrY6JQERUrG3dQcSfDUAzBzlnEki8cRJGB0J93Tv7sP1LxvP5xY+RMw8ErXEstEeHy/J87JAIdB6NVM6Twc+e8vmRPcY6ShxwLM6NiwBN/HfdDf9+Y1ZjWUz+12mkPxFPO/sw9pL9LqvsfOmupqDXA1xYQhxLSLXozqlG3L3yszQijoVXzVHu9JW+o0XsU+TZYQSL39niRmhalMnlWcx86ELfd4Yx8YAXrrx6mYjIAVoSpES+tEX/n/2TdYgmELpDVFX581iytrB09cK304++3X3G4wE/fF825vkfMM+Toyq8pWXvF69vs5yMheV/FWfXW9vhV9NXS/rlNDLiv7v3EfQZs3Ta4z64PjxeZyDnJObmZQ3hgvcbI5j6dRqp3yUQnx5BqDeD2MviStdY0OJzaFd5GHxioiy4SR8ZwPBRNSHUOhZdX/bBf7/Kcz2PfeiV88zHR86Th+Ku0rLydd9dxjrMnHAsJv950ei+2RZWc2w4F0f0lTT0oyDeN/Voc2HW8C/m4Lspg+zn+5H4sfnobxz3o1OIRfvQcWsPQgcnMV4chxPC1M8mMPHciD7Vd2AI4W8EMfSDiD6N748j0usWZU0FqE8E4RPHN8fbj4nIQVoQpATR3VlrPIqoADtcwKefiutKizeGELgviNk/qenLS0g8NYTQfT6Mn5F9JsInaUw/JYeVjiP0ghzvsYLUT8TV4bfHjddN4hdy1QeKWuyKziD1+1QTjxmM7VRvtuNcDPNyn9q3oq/YyqBh7+15pEQAU0/8yQimT2exqlo/2m4VlUhvAIMvxWpXmK5u9P9wFKPiMbTPB3edrpJax0If/PvwrN5FIq2+n8DQUyH47htHSh2KlTPTGHpDPPnHEMbnxcyVFMbFsQr9o/G6md1jsaFEXi9dEnl4x71Q1bIt2VcHMPq2DAdd0B4TAWKVbrtaxr/Xh/6H+xH4mg+BZ67EzcZ9GApuwcKPQpj4ZJMIydpwS+HGsd29+KI43CvZeX1y7jfvImOM2Db8VBxfy4BrTetHVL1MROQE6w9StmnYIsc9VBuPUnD5Yo0WhCzG9Ste4eZu+A/IJwHs8qozramiD+/V0H4uXbu7RLcJmxsM2Dw50i8qXxHo2H70Y/Qd9WZbCvvUDm2vuorf8wh6Pkur79KoZwmxQXm3UAhDL8SQOL2IrOx+uEmsa38UkWoVZl4EcoW0+qLFoMLghrZDK+92qXUszomA5H2jFmvr9mNQPtmzC1sLh+KuPnFkpDAC97QjO/8a6g+zbHAs7vSVWmlMj80iyPpcR+X8vvst+1GXG8Ef98Erv6NNlCufXq7sSz0zjsSyyAuXhsjzI6Y7dxzmnl64c0lMnBLp7Ba5c2kZ868aL2k7PKIE5vHxH1WwdCqK4wt55D+R7XxERNeGdQcp+kBB8b/qeBQ7Xn5Xv+KVvL3DcO8PQGvPIqNX6Kqi7xyGv7sNS6enG3zvhQu3XNXLd0Xtk7yKl2FK4H43Vk6W3ZRdqTOIyP7CcM0lpN6YRnRQBARfG8KczAtZ2T5kvFpbArmyMQVh/ODgY7YHgU6fXjICxpu80J52G4HhhYw+VgbtGgL7xfF+2o/uTSJ9DVqFGh2L4MPfRfjb4YpHT7sLXTsr54f3B8u6z2pzI/RiDEO95zH9M6PrRi9Xxos2pRD96W/1loe2zh7s6laznea9KELhSWS37UWPCGBX//Ru8ft5tv/fYo8/y2JJBS1S++ZVfDxf7VPqhW+3T/wlInKW5oKUTg2h74TKxj0EumqPR7EnhsR76vL/Dh8mdnejbTmNoTeNFhZ5VT82IE+gS0i/0GgYYg5Z2SVRT40r+NqPXeWDWG1R+yRbh56OINCZQ9I0NqSqHbsQfGCXmjBLYfQtlRfyDqoGokE/Bo6piUfd6FjJqu9hseFoAml1KLxfnUBABIaZ+SEk9O/AaUP3rjEMbRdV2Ud2WoXqH4vZ58J614j18e5f8jj7q8r5/Q8P1/yCvBIRoLwUw+C2i/jt4QHEX30LC3J/vtTTVLeNbnlZBHwrSP98COOLat4arO8HBt0IPtT4Nmr3A16x5CqW5gs5FIL3NvEvmzEd+zDuvjWLxULZMHt2DBOHRvFdW98rQ0R05TQRpGgYm5zC4BODGPt+oTKt9/0ohpW8qOBuFSdbNV1N4ljhLgg3PKIyWToTQ7ZQYYqr+r77xSl4MVn+BXEW2t/a+/0V93/qhW+br4mHBm9zl+G6xNtnIZPv3R1E+7m3Kr71tarOrRiuVple+isui0ro0z+raZvCX70b+NdlNWXnWCTU3UnCbR5xNJYw/8ssYsfleCDhjj7s6lzFYoNWIbvHorUKAUqbHlgYX82fwMklkXJRhnofa+Yg+jDyYhgdp8cw8GqjwLieEYy9OIbRcJODaOUdVJMi0P1dAsPfatyM479N3ryew/nioNcuyOFH+T+bbl3ffy/c2XchvwO3wivPY/ipQQxz0CwROUwTQUo3tsjxCatZLPzxpD7Hd3Av7pYnwzPHEa1xZZ35JA+4NosqpI4z00gVBtBeWkRSbzFJYO59vWoUxBXt8frX0d5b9VGCMIYJ1pb91SRGnxtt4jGOeMMv4vKi/T9uwmZXX2k/T7yGtMwTVx5Lx2x+SZaoTAOiUusrC1TciOy6G65P/gXJl9UsqXMTaoYCssXr0AxC97Qhd6EUHtk5FumXU6UBtCIw1FtMTqjvf5FW0kg0aBWyeyxaR95OG9d/gydzYrQssEgcW9Ar6vJvha1HrmsU9+ZiGFr37xrF8PwzQxj8QZMdoR8uIHViGrMflvXd1ZT7N9nSBWxSB9b9+Bch2zc3bSo0vfkwvrsdC69U+QzJlkWvuDAo/kgiEZFzNBGkzCH5fh6rfz6Pi38XwcgvEuLEtwW5+ThGn6zdTjA3/zHyN3XAU7cpOYv4vDEeYuW9RLFZf+4N1cLScMCshp4trrWPi1mH4ItzSKfjCN3ZhvadY0j8IYX4D+UraUy/lxFpX8CsHuQEMXE8hdQfCr+740FALJs6PlFq2VhdQjrbg9HXk0i8MoXxQxOIz80i7D2PhOy+0BcaRlzedTSrAiKXhkG5nsLdSKk00r+ewuBuD0SOIPeh/iadrWNxLq5/CVx5YFj4/hdxpBoOmL0Kx0K/nXYTsm9PVAYWZ2axIAMtEbg9UvGdLpV8B8Xx2PRbjNn8DR9J/pJyZZeOHOcha38R9Kk5tn2Qwtzbjcb8lMwdEcdsxY1dP05g5vUE4g/+FclTIvV3BZF4fQYz/zQK9+lx/ReZzdyPTmBmJIi+x0U5a+J7YYiIrpQmf2DQDe3REPbeLq7QVsWV8lsJJBr+2mwYsd9H0H56AIFn7J94m7JNBAcv+bHysg/hRmM/nGpbCOHOJGLy20nF1W34633wiGy+/Jd5zP4s0XxFV5Xzj8XIbBLu35jG1dgiAoL7NyNT68vTRICyq7sdq5m54peuVeM7OIOxuxYw+nC0/KcJ6vJh/Pgo2n7eh6FCd4n8ocgX+0RE1wbtrhymvyYCTJWG+h1heWTeLAU1oekkBjuS0IL2bgyW4188LtM6ZCuJKET5qvsdxtQrW/Hat4fRI3+ledMMtEcaDO4mIrrC1vkryPb4xBXexJcziPYNNbgKXxt9/f9pCUN7h5uoXG5MTj8WoegE2k8MYfIK/9aN/qVojwDx7zXzC85u9B2awOi2LMZEfhZajsLTcWw9FsLwXTGkHmvDzFdCmJTfsLzbi0IHTHU5LBydxKzafrNBSnO88N65hKUPgpj63TC2nA4g8JzdtiMioivjigQpcoDt1O8i2PxmEKEjLT4RdkYQ+59B/PWXoXUOcrxR8FhU2DGCmUM9WHiuXw24teHOIMaei6DvSy6snBpF31Olzi3vnV5R+S8h+FISw+4UAt8ctd11ZLaxQYqybRyJl3qQec7UEkRE5BBXKEgR1lIRNORGJBZH4FIM4SftjyG44fFYlHSKoC0WwReXk0g3SvStW+D9wi3Y/PktaHfJu9qkFaSqVvAaxo9PoedDFcA4riVFORBHes9FTPoLY56IiJzjygUpgvuhcUw8eBnTj4zCuD9ofbp/GMPoF5IYeooBSrN4LAyhyTlE7qkfOtS1ksLYN4erDBIeRPwPAVz8mR8D8gvVrsCYlLWIcDwKETnYFQ1SiG4Yj5vGo6hZtu0fx8z9Xdjc6UH7TXlkl3NYWYwh/KNW3y8VwlRyEB3vBNB/mGE+ETkPgxSiDVBoBQkEo45rWQqLtEVuP4vJV8Tzf+hA8ql+RK/wQGUiIjvW/ds9RKTsn0IyncTUvhC2ezch895bjuz6Wv00j/y/bUFwL5A4PMQAhYgciy0pRK2ybxyJx7xA/jLOn4ohemSOY6WIiNaBQQoRERE5Ert7iIiIyJEYpBAREZEjMUghIiIiR2KQQkRERI7EIIWIiIgciUEKERERORKDFCIiInIkBilERETkSAxSiIiIyJEYpBAREZEjMUghIiIiR2KQQkRERI7EIIWIiIgciUEKERERORKDFCIiInIkBilERETkSAxSiIiIyJEYpBAREZEjMUghIiIiR2KQQkRERI7EIIWIiIgciUEKERERORKDFCIiInIkBilERETkSAxSiIiIyJEYpBAREZEjMUghIiIiR9q4IOXgDNLpJKb2qWkiIiKiJrAlhYiIiByJQQoRERE5ku0gZSqZRjo5hZCaBkYwk04jOV2aY3TxpDFzUE1Lt08hKebJ+RWv6Yz1FF5Pp2fEnJLQdNKYp9ZtTsPIrPl95es2XmN3ExER0bXKdpAy8E4GcG3F9kKlv8+NDvHP5d1eDBpCt4k5+TSOH1Yz4IK2E4hpGjTxmJzPw7PHFDjskwFMAB3zk/rrxjIdCFgCFcCDwM4cJuUy/gHExRZl0BRoTxvz5ONIGh17qgVBREREdC2y391zeAEZEXRs3WGEJKEdW+HK55EvBi4hbPe6kF86LYKIgjzSR2VQYYhHkmXrGHlQg2s5AX+k9A5jGQ/85hYay3pwcC80VwYJPWBRjg0guSzCmZ1GS0s0KIMXPwaOGS8TERHRtaWJMSlRLIggwNXRJZ4bAUnmnRjO5l3ouF3M2rcdW115nD1VCjjqC8HdLv51Bcq6bNLpgAhRrHLImoINvcVGtq6UvS+NgEwaERERXReaGjgbXcyIoKIHI3pAksHC4ThOL+Xh6R5RLStncbrJlou8qavH/DC3rlSVN3X1mB/m1hUiIiK6ZjUVpBhdPh70PNgB1/IComJW/NRZ5EXgsrfD2tXTSBzZlfIxLXbFL+TKx8cQERHRdae5IEV1+Xi6PMjnxBPpWBY5Ebh4uprp6jFEf5NG3qVhcNY0TFbdxVN215DV4eNI513QDpgH2Kq7hNTdP7y7h4iI6NrWZJACLOfy4q85IDECF+u4EVuODcCvJZAxj0vZ04H0kUbdPXEM+DUkls3jUtRdQuzuISIiui78jcfj+Xf1nIiIiMgxmm5JISIiIroSGKQQERGRIzFIISIiIkdikEJERESOxCCFiIiIHIlBChERETkSgxQiIiJyJAYpRERE5EgMUoiIiMiRGKQQERGRIzFIISIiIkdikEJERESOxCCFiIiIHIlBChERETkSgxQiIiJyJAYpRERE5EgMUoiIiMiRGKQQERGRIzFIISIiIkdikEJERESOxCCFiIiIHIlBChERETkSgxQiIiJyJAYpRERE5EgMUoiIiMiRGKQQERGRIzFIISIiIkdikEJERESOxCCFiIiIHIlBChERETkSgxQiIiJyJAYpRERE5Eg3dJDi3bELWqeaWK9ODbt2eNUEERERrZcDgxQ3tG9EMHJoDMPfKQQRbgT3h6Dpr7eG7+k4Jp/woe2cmrFe59rge2IaMwd9agYRERGtx994PJ5/V8/XJPhiAkPb3Wi7Sc24tIjp+8KIqUmrwOQcRra3qylg9UIKE3uHMCsnOkOY+FkEWlsG6TMZEZv0oOeOTVh4Lwet+yKm/QOIYwQz6QA8+rsbyKcxqb/HYt8Ukk9sRuLhECZbFaRInRHE/mcQf/1lCAOvZtVMIiIiWot1t6TMPhWA7ysaJufzWL20Ctzsxb2PqxcrhBG43QWxlJBBQtPgKwQocGPwJxH4kMJwXwhDz41i6HsB+EfehbvXizZ9GSmKfvE+7Ztz0MMAGYjIadMjEJlE4oM84OpAl/4eswAmvq0hf3KytQGKdG4aI29moT02gpCaRURERGvT0u6e8++lsSLCie6vDouQo4r99+KWD9M4rybLdIbhu6MN2f89K8IUk1NRPH+ySqvEucu4rJ5aZefjiH47jsVLHXB/S81U3E8H4bt1CamX02pOa2VfSGLxJg3BaJ3OqYMzSKeTmNqnppsiW5LSmDmoJnUhTCXTSM+OqGm6nozMimObnLrmAl893SyTRLQO5UGKXnmKE4t8rOXksjKHsyvi/x29CFcZkDq4qx3Zt3NqymKHGx3iX8cXeo1pk/SJJaPVxIbAE8MIbZPPYkh/tIrP3arPVtwIb/MCH6Ux3upWlCJju+6eYO0xNIf7oWl+DBxT00RERFTBFKSIK/Q9ppEeXf41XOnP4bV5GU54oD1uqaI7h6HdtIDXTqhpq2MiEPkMaOsOIv5soPyumzMJJN44iaSarKd7Zx+2f8l4Pr/4MXJinSVBbBXrzX5odDAVbQth+NAYxkyP4UdDiFTMk/sUKJ//RMBYh8n0R+eB27zY26o7h4iIiG5ApoGzlQNSMyc09B9WEw2EppPYe8GP/l+K9fxarGclhdG+IRG2GLRoQmwhisDIA2o7ckxKv5hTEnopicFtLjUFrH6SRebDNOb+ewxxPfgxU+nNLyL+wgyWxByXpw+hR3tw/qc1WinkgNkDW3H2iOX1b00gMaDBfbMa+fLZKrKn08j/vQbPpjZjULA+bwKBpzyI/z4Ar1x2dRWrHyXg+/a48b6CwnZkOl5V88xki9WeDqSL6ZDdNYPQirueN71mor+vfMiwcYzU+1dEMCeCqEBhIE6VgcPyOA32lvI4Pz8Jf6RiaLFQY53LCWjBqN6Uv77tWMubuTzY3x+zsjQJFeXXkn/5+QTOekVALLYj96mQJljep69XpMRYRp9TJ+2Ffc8hIQLygNyeKd0N06iXHQ3FnBP5redBe519t76nLJ8q89I4Fl36PnQU8kC8uZSW+vtXLL8nzmLrHrld62fZ+n5BlRtd3fQa761MV+V8ydgXlH9+zNsiomuaqSUliv4TGfVcEB90uwFKmXMxzP9J/G/fir49xiwRomDv7fmG40DiT0YwfTqLVdX60XarG97eAAZfimHq0aqjXERk0o3+H45iVDyG9vngLtxlVM3tHeLEmMenH6npgjeGELgviFmZbunyEhJPDSF0nw/jZ2T/lfBJGtNPyRaYcYRekGNvVpD6ia8yQJGOZZETW+ooO0vXNjIrTrAQJ2o18HdyXuTYgSpjVvRuooSoEowTt1y27Bh1BdCzWBhALJZzaRg0ddvJClKvPNV25DK53kEkp+uMdjCv84gI3MS07A705ybLthM2raPxdgoVTmEdGhLLHgSs4y4a7I+ZDAwCXbKyVMuLsuzZYxq7owKUQr7JR7KjVNnZZzPtoooO7MwZx1SvgGWwIAIUGWyo98n87DCnUVXeMK17MucvC2oqyP0S78mZ9iuxIvKpSl4Wjpk5WHSJz1fHO8b79LIk02DZv8n5DgQqxlC5oIlAJakvU36xURjcnlgWT2XAIJcpBA0201uRLsXV6weOqveKY+wS5SqdHiwuWyij5WO2iOhaVT4mRa8E1Yd9zVciWYz/8yJW0Q5tb9iYtecR9HxmZxzIEmKD8m6hEIZeiCFxehHZT8Tsm8S69kcRqdZ9Iq7CpkUw4ZMPXxQpFVMY3NB2aJZBvJdx8Yx6WqaQbuHmbvgPyCcB7PKq26VNQVd4r4b2c+naXVfKpr+tM3i2KAS33MRKtniVHI/4kZjPiaCqvNprqCywjOL4fF5UTj2iWhVE5ePvki005grFWMbVu9dYphrzOo8NICkrHpHnsWJFF8WCmOfybjcqGTvbERXIwpFJ0zrEZnJV7saqtz8WXR0i2sjnIJOnE2V5cl4GxXJpESDsFBGjJfCOBo2Aryl20y5bw46aWj4O7hUBkQiizK0hKj89O40KeuRBDa6yvJVlIYa0WH11xn7J1oSy/fqNqKhFQLfXXFFb1ltkyZPaaRBByYPlOZ85YQ1OGmkivZZ0FeTnY6UWxsPHjbwpK6OncVbM83TXLNFEdA1p6d09RS+/i6VLQNsd90KGKYH73Vg5OWm8VktnEJH9hS9CW0LqjWlEB8MIfG0IczK4EYGD7yHj1drEFXvZCT2MHxx8DLa/Xk2lW/L2DsO9X1xpt2eR0YMrFXR1DsPf3Yal09NodH+Qy1XvErggjtNLItGqhUI+5FVgNNKPgcNVKpW10luRREVzQA2MVg9zl0xL2NnO4Siix+Jiz2WrRGvSEV0U4YZsaVHrk6028ciAqLxkNdoFI4YphjDKsqW82GA77TlkTd11odvksHAPAuo9hUeplaQyWDXEkS0LvM2M/TJaE0zrNXelFFSs11CeJw3S0O7WgylDHrkP1VPb7Ke38lgR0Y1oY4IUxJB4T5zVZIvE0xEEOnNIHlUv1bJjF4IP7FITZimMvmW0cLRVqwssokHTWI5H3egQJ1zLMNk6VLqlO3yY2N2NtuU0ht5U2+/2Y2zAB68IotIvNL7fKPcXe1uWLSdGC5ZxZS+7KeTJu/VN1qbukLJHs1fEjTTYjt6lIPfRj9wR4zX5PTvrUmwFnNSvrgsVYd2urLVYT9rzpq4e86PWWBObzF1Y5ke1loh1qfq9Q827YuklomveBgUpQOLts5DVvXd3EO3n3rJ3Eu7ciuFqXTqX/orLIkz49M9q2qbwV+8G/tV0RfbnT8VaKr87xSxxLK1ud3bDI9KydCaG7NEE0nJnbvKi7343VheTqNsutG0zNsn/ZXcW1TYyXeiPV19UpyrajttaWMF+mBPXvh70tDzwsbCxndCOrXqXwmQLb8MOiTw0GvjjGPAblZ4cE+HqkNWq0WJiPDczruytyvNdtS4oa017/EJOJGArtte8Y65aa4VUvv1yxn61rmujQRqWF9YZzLY6vUR0vWtRkOJF+3/chM2uvtL4jxOvIS27SVx5LB1LGPMaEUFA4MUx9JUFKm5Edt0N1yf/guTLapbUuckIBKrp1BA6NIPQPW3IXTCFR+cuigrUhc01xuDqzkwjVRhAe2kRSb3FJIG591ULiwi90sdrfem/csctYisryP5RTddzcAaBXsvAwX3bsbVBq1HTAUxh/MMe8wBIYzBnOj1TdZzHmtjYjj5+pOyqfAR719Pds28KYZGHgbL9GEGP2IBs4wjJwOWdjNhw+YDKkVnLHSiFYKYwvkYITYfLBteuOe36+AnZDVaeRr3LSB37wtgM8yBk6/bLlfbL3GIkBxGvtSWudhrySP9mDSFKWcDT+vQ2JAfqbkSLGhFdEesOUoIvzomTQByhO9vQvnMMiT+kEP+hfCWN6ffECencAmb1gapBTBxPIfWHQsXgQUAsmzo+IV5RVpeQzvZg9PUkEq9MYfzQBOJzswh7zyNxuNAkPoz478X7ZlVAJMchyPXIefKREif9X09hcLdHBAqWfvM35/GxqIQ6OvvUjGqyiM8v6d07K+8lir9BNPeGamGxMWBWu2uLiM0+xvybakY9ejfFpMit0niKwt0P1W8NLgxCbb47Ixos3DmktpM2bk1tdXdPo+3I1/U7Ygr7q3edyK6uanfJ2CACI7/ecmJep3GHij8YNcqNzGd1x4/xeho9i0aLVYlshVF3Eall5K9Q6XepKGtPu9HCUy2Nxe4euR9H0oBpzIZ1+xXkflneU7izak3dJ3peGndjla+v+VavQsCj52Uhb1qdXiK6rq37BwZbZlsI4c4kYr8SocCdPoS/3gePuIK8/Jd5zP4soX8PSiuEf5FC5AspDOwdbjjwdW00jIkgyb8yDd/3GrS40FUmW3hUAMXv1SAicpwNG5PStDNxI0CRPkgh9sIoRp8bRbSFAYoUeyWF7G09eKT4HS4ttiOIHncWqVcYoBAREa2Hc4KUK+XUMGbPtEF7eNDy/Smt4EbksV64/jiLiVNqFhEREa2Jc7p7rigfRmbH0LM4iv7DZb+5vC7ux2OIP/hXxL43gPiG/YAhERHRjeHGa0nRpRB9ahrLd0UwtlPNWq97hhG9/yIDFCIioha5QVtSiIiIyOlu0JYUIiIicjoGKURERORIDFKIiIjIkRikEBERkSMxSCEiIiJHYpBCREREjsQghYiIiByJQQoRERE5UkuDFO+OPvTt8KopIiIiorVrYZCioe9bEQy/GEd6bgIhNZeIiIhoLVoYpKQx+WQAsfk80O5D3wE1m4iIiGgNWj4mJf7mAlbEf2/vMNzGrDpGMJNOIzm9tnaXkdk00rMjakoKYSppnXc9aNV+tT5/QtNJpJNT5S1nB2eQTicxtU9NU5OMz8XMQTVJRHSDKg9S9MpFVGLysdaK7MRJnJVRyh29CHcas2qLol/T4I/E1TRtrDgG/Bq0YFRNb5DD/dA0PwaOqWkiIqI1MAUp4uptj0c9F7r8a7wSTmDufRmleKA9rhmziIiIiJr0Nx6P59+Np7KJOSBCi5LMCQ39h9VEM7aNI/HSLrjPzWHgm6NIq9mVjG12zE8WW1NkF06gS3+qq56GyrRiOQEtuIyp5CC0lYQIlQKl9eTTmPQPoKy9Zt8Ukgc0uNRk1WUU2aUx2JtD4gQQkIGcadmG6bVsJ3NiErmdRhqNFg1jX2B5n75esRfGMrKbxvweyZoHGSS0fhRerZ7m09huXo9sOTMHpial/ai1HZWmYgYKhXzR19uB9BFTa0qj/C6858RZbN1jzq9aZbBQdhI46w2Y0lGeD5KRF6WE5k3lrdp6itu05o9exsrz37p9Y90oz5uy9xlqlpsqx8ScB3XLW0UeVuYFEdG1xNSSEkX/iYx6LogTa/XKwYYzs1i6IP539iC4zZhlh6xMAl3yxKpBkw+RHs+ean3zRjdRYlk8lRWAXNZcCXQF0LOo1qElkHFpGDR3X8mTuagwc+IEbywj1rUilrGOrSjjQWBnDpNyeb1yNcZ3BNpFZavWoR1Jo8OcXrUdiIqrsJ2FbkvFviaFCrK03sSySF9F+q1pttC7ZVTa9cck0nkxXwQQx/VjX287RtfRpBwoLQMO+Xq1bUi18js9I7Zg5oK2E4ipZeS6PXvqj21x9Yr0vWNar0yfab3FYE29Lo8RegcrypR5PaVgQQZahXWLciTKlXX8lKvXDxxVy4jy6hLrTqcHS2kS28uL95W216Dc6MdEbEs8lQFIMT12yptO5KFId1JfhgEKEV3bysekmCsty5VfM3zfj6Dn8/KZGz3f6tPn2dHVIWrvfA4y9tCJ9EzOy3aYJsfHlAVYURyXFWlXj1qLONnv9OhXvOYgLPobUZmIYGavpfIqySN91Hzlv1cEGyKgMlfMxwaQFIn37JSVuLEdmRbzmJto0KiA1kWkceHIJGKm9S7nxD66OmC6yBYsaW5gZFYGUKZ9sr2dekr5UJbfej544C+r9MvTG48kxTIubN1RO3Ssv94Qui7EMHnkeKmyPpZFTvzruM2yTst6RrAg3hczjatZhrHrlhyeNy1z+LgR5JnXdew0zop5nm5VhhuWmxqaeF/mBIMTIro+tPzuHt/TcYx96xa8+5xRGbff1YeA8VJD0UXxDtnqoQbvyqvWeGRAnPBbecrtgoyFjCteNUhYPsxdEVXlkDUNBA3d1iH+yqt20zrEo9QUb2wnnyuGXIpR2a2LyI/osbiorGRLh7Fdc3dGSXma6zFasUSQcMRUwdneTj3188Fa6Ter/nrjiB4W6Rf7IPfPOEaWbkLFup6o2Hf5Pr1FRX9fK1rA7JSb6uy/L4/ch+opEdE1rqVBik+c0McfcuPsz4cQfSuG+T+Jme1bsWuP8XpDxZYco9uhEEis9RblegpN6daH+Wq6oUI3h/VRq9ujVeT4Dr2S8iOnuiP0bpe1EsdNBh9lrQJSq7dTTbu7dutBC8gxHLIMDXYk1fGx15JVDGoK3WWFrrBWWGu5uVrljYjoKmlZkKIHKHu24Pyboxh4NSvmZBH/45L4346e3faqodD0lOqSUbfKihOwHHey3qvtcsaVdrH5fY3iF3IiYVuxveZ4iSgWqqbdaFmwKu9+CMHdrp5WEdqxFS69wmrBbb4yEJEDNS3dUlJrtlOrxUS1sCydXlcFW3O9esvICHrEy3pA2lT3ZQjbvTJom2x5ANC43FS31vcREV3LWhKkGAGKB5fnpzF0OKXmijDlhRQWL4lz611+hNW8mkRlGe7VygY9FioZeQFbM8xp+kpcBEDviGtpyyDIwpWzdUBlTfr4Axe0A+Xp1btF1ABWvfvKsp2RWWt3g6rEvduL+xGaDtftWtDH7pSNCxnB3qa7YSSRXtnNJQORKpW47e3UHaNSym9z3hr5kEHSEhg1rd5697khO0nMAWBo2m/J/2qMQKcsANLHhKjn62Gj3BSUBa5NvK/EGGybrhigTER0bVh/kLJzHKMyQFmMY1RUDLINpWQa7/5pFbi5G/6nG3z/7LEB+PWWE3O/u3FniV9UoNWqssJgV30MS80TdRWyW0nd5WFsR461MO4Asd/dY7T2VEtv8eq7ynZ6Fq3dBnI9CeMOJLVMGDG9BamWaNC6XdkdYwwYrbzDp46DPUaFbdp24SErfjvbMQa3FpapURmqfJB3ohTWb9ylsv4BnrK1Y6HbtF797jC1XlmmrMe5I6l3WcmuxNoBqbx7TOynCIAK79O7feTdb5ags3k2yo3YvhzsXd7daed9RETXF9P3pKyVG9r9Hlx8OwXZuVPJC99uD9r+soCT8+UhzHVv3xRmdpxGf1lrgby6HcTWJfN3dVDzZCuCCmKZj0RE16UWdPdkka4ZoEhLSL05d+MFKDIY2a/B0xsu+54PoyunBd0cRERE17mW3t1DZkbz/OQ8oB0oNM8XupX4PRZERESNtKC7h4iIiKj12JJCREREjsQghYiIiByJQQoRERE5EoMUIiIiciQGKURERORIDFKIiIjIkRikEBERkSMxSFk3L3zfGcbY02H47lSziIiIaN0cGqTI3/vxib/W5w7TGcLU8XF81+vSf3hu/JcpzBz0qReJiIhoPVr0A4M9aG9Tk2uQz8wh9YGaEOsb/MUEvDdtxt23XkTmsxyy74tQZUcH0s/0I3pGLeYAWjSBid7zmHluAJMiXZFYCuG7Moh/JYRJtQwRERGtTQuClBCmfjcI7Vbx9NIKMhcuGrPr2Pz5LXDd3Ia2m9SMP80i8Mg49J8g3DaO2HcWMPNvIYx5FzD09WGk1C8HbzkzgMAzaf0tTqAdSmBq9yacfK4Pw2+KGQdnkN4jgqkX/Bh4w1iGiIiI1qY1v92zbwrJAxpcyCP90xAGXrXzi8deBJ4eRPhBDe62DBLf7Ef0nJjdqUG75TweOJyAP1f4Gf4IYr8Pw/2/RtH31Jz+bifSW1K+tITp+8KIqXlERES0Nq0Zk3JsAKNvysDEBe0fxjHYacyubwmJFwYQ2D+N9CceaI9rxuxzaaQ/COCLn1/F8nsyQBH2daPr5lVk33dugIId4+i76zIWj00zQCG6nsmLMvmr5rMjagYRVROaTuq//j9zUM1Yg5YNnE09N47E8irQ5kX/iyOwPXz0gxgGfprCpruCUGGKCkpyOP/PcsKNyP13Y9NHv8X0Uf1V55EDaJ/twcqxUYRfrtUdNYIZcbCS0yE13ZyR2cqTolEAZsSab2CFCmOdH4RrjuxaTCcxtU9NXy3V8t86r+m0ru+zUpOervXmmfis79fgWk5AC0bVvI0ku7pbFBCp41I8TleqDFXL9w3Z9lrKjfGea+/cYSfdV3/f4hE/Jufz8OxZez1VHqToBcc4uTT/oUgh+tQMlmSc0hXA8KEm7nI5MY7RqQQKo1m07i1wXXLB+8w4xl6aQODmFCZ+EIVzRqOYdAYx/vxefHo0jLAItvqeHcPgPeq1MlH0a5rqvqLWMFUYIm/7D6vZdIVUy/8q8w73i+d+DByTr9vh3M9KaDoMzZVH+jdXIkDZYE0fF6fjOdaJ4pEkMvAgsMZA2xSkiKhrj0c9F7r8zUe55yYx/PM08uKpe/dYE+/PIv12Ckv6czceuN2N/Psx9P84geT/GELfw8OYleNVNpB3TwQjh4YR6nWrOQZ3rw9aofvqziCGn42gr9id5cPIT4Joe/s1JC/1IPjECPr/XuThe+pl2mBd6HAB+dyymqYrq1r+X8/HJITt8usGlpPXUcVOtNGiOD4vooKunjW1ppgGzsqmoYCId0oyJ9ZydeoWVxtxDPaKD/MnaUyGBxBvKsAYRPwPIeANDaEjatYG8x2cwdg9OaQ++SL6vDnEfeoW4k6RJ78OoOPMOPxPzmLwtTRCd4iQ6m3jLqO+5+cwtrNdX0eR2Ofxrw1gVk2WGPnbMV8YDCzIlitzYFi1CbnyuBSWk909g705JE4AgeJ6xFXeEcvVkWxu1Qc2K3lxXPziuKjJcrJ5eVBcLarJsvUZr21dMu2DoKfDe1atU71/JYEEAgh0qYVUmmW3VXFe3XRI1n3PIKH1iyIvWPNOqFpeG+WxNW/E63q62wtps7PP+pw6+Sbo6ehA+sRZbN0jt1faF+M4Ft+IvLmM6KzrFu/Vj7lYn/VYF1nzrrJclB0LoVr+1VymWv6fE9dLnVWOCdS+m7dvzXfzsa32WalXFoRGnwVrHptfs5MPOpVmVByf2usobLdsnYV9N5V/6/tLZaD0eTLKrd3yaF2n2F9V9nJlx7B0XBrlYZHl2GVOTCK305zGkpr5frv182Colve2j4+13FR85gzF91crw6Z1W9Ne9rms83muX7alemW5dLzLzp/m49Ag3boGy9TN0xr7tly1fKi0l22vSplRr1ek0wZTS0oU/Scy6rkgTtbNrsyQRTwyipMXxNNbNYT/W0iELXYNI/ZPPrQtZ+H+RgLju9XsDTWIyPZPMTs4gKXLbUCbCx2FlpKHtuoFKfuREXLMzhmtRAVzP+jTm7TLHlUDlCpkQVYHzXhvApmuQJVuNqMJMyEvTGUFK5ctOxF4EOheKG4/seyCdsDU/ycLh/jAyBNTcZkVDYPJKfFxqDQyKz4gECdPtezkPMT61tB3LPalZ1Ft84jIN7lv6bR+x5aRDrG/LlE+avUfy/xRJ5xCuifnOxAo9GPrTdViHeKpPHnI1yvKa6M8VicTWekUt5Hzl3147TFVJIX16Plm7YcVx0Z8+JP6MsZJSZ4s9A++ep9MY6530NSvrtZtOibakRz8xRNCFVXzznwc5TrFSUoGYsV1ptGxx9yfX3sZvY+7Wv5/08YxkaqVyWVRjmuNr6pZFqzL1/4syL5xvRzqJ1D5eqliDnTJE61KizgHegr7aHV7h17x5C6UgoNG+VTZJy+WL3SHFYPgyvejrAw0z6iETPulJUWazBVnLfbOJ+bPzEK3KJ81Vlwr3w1i3TuBmFqPkU/mc02DMmhLnW2YynDhHFEor40/l1Ll57lh2bZblsV5qnSutHx+66S7qOYydvO0yr7pzOVjEum83Lc00jtzxfXpZWa/pX75MKfXnZ7usr20pXxMir5jKuGWiLg5KQw/LzJoVexqbwQTzxaHxDYwjvDX+9H/sDhA9wWM7x7ZaN/xAosJTJ8Lw3+X+KSdW8Bx1fIT6dZDFGR+ZUxnX40iJV7L/6UFo2P0E14euQ/VtAxGRGGRa27uMIoTkelYRX8jTwge9OgFThTInR69wjAXYn0ZESDsrfigi4BSNgytiEDTmKGfZBLzOZHeJk+Y5iD32ACSMsgSV46x4tVfFAtinsu7vbwwKyMPihNq2fIyLTHxoRAfgAdt5lCDPK69DTVh174uZI9OYtI0TiF+QeQZOuAunnANmROmD7w4Yfm75MnbfBIwmkZdvXuNcnBwrzEG4mjpClnmZ0w2n9ZQa78SJ5LIyuNYZ53l2xVly3RlXjiOnp3VA1x7jDJpvQiKBsWV44kFsV01w0TfH7G8ueWg0M/tL6s06n0Wqusy+qZQ7JwS58DJ+eqfwtBtHeKvuTwJNvLJKFMqrYXlC+msdSxOJJC80LW2fNbLlaWsybJvvgitqfH5RB4787GQx87OmiuV77dxTF3YukPtdUvKYINtVGPnc6mU53Hjsm27LK/33FdLE3lavm8F5vIRx8A78siX53F0UcxzbcV287nvWBbyjLgWLbu7p8KpKCbfymAVbdjSvQvdarbj/I8BhH6QAPbfC+/NIhB5/7gaoNsHr7tNVNgZPTAx9MC1KYOzKmgp06lh1/2a/Vajwwv6h0U7oAYqy5YNUVgGRAGoLBhrZYwPcIkrgOKAaPkQkX71C584Ti/JvkOj1UM+ZHQdjfRj4HDpA7PxKoMlQxzZFfGv3W3vBFU3jxtsoxnHxPqOxRE/Zlyl6Nuq2tJhqeD0IMqUPvUwNzEbFWMOWXPTqWAEQdXU3q/oYZFOcRzrr9MIrIxl1FWS6dF8K5NVrTErMm3yoSaL1P6YyqTxsHSBrpFxQtUwqNYrr5TjkQFRydj7FNrLJ3Ey9xtX4kb3SunkX/NY6HkRtRxDmyqCc0Vdza5drWO3jNz6VlzVxpXBBmx8Lg3WPG5Utpsoy+s999VgP0+rlJ+rZOOCFCGTER+KlTSmfzCORTWveev97R4fgg81bskJ/X2XCKeyWDqhWkk6ffCIApXPzKP47SzbxLzLy3iryhib4A//EeOHBhFU040Z3Tiy1Uo2QRZPlDW6Ydaj1N1R/jBH+wV686z+unF1JJu+ZSG237x6Bbg6xOnAjiuVx7KPWebTIDreUflr66pVElcmKo3lj2pXMVeCCx23q6fiaq7YJGx+mK/CrpBC95H1Yb4iXZNi67FsuhZ7rwL6al0tRhBnyp8CO/m0z42OfAYZcTVc8X6q7aqVwY37XG5YWbbrauSpLP/qabM2LkjZMYKJxzrw7o/Ejjc1cNbi2TFMHBrFd5scn6I9Oozx6RnM/X4Ckf/cOMQxmn3PY6Hw20A72sXpqLz/WdvTA7w/q1pays3+bBTDg8P2f7Nn3xSmVMVfDAxkxWa7ArbDuMJpph9wZLpQgRcqeOPk3XFb6aTt6ihPoZ53LVXrqkFdiSwv2DtR1M3jBtuwqLvPB3vEtYlxUqsW+NWkX9nW746o1W1kXBFVU3u/Qmod9deZwYLYB30Za5NtSxhl0pqf5vSVM/anVrfgeoVEeTc+HbK1wzhZy/FflekTVEuE+bNgL59EECvHKbzTj/6jsv+/NO6i5rHYV39v65ZHPZ1VgiG9hWA9VBdtRd4YLQittnFlsAEbn8vqGpXtJspyrfNSRQtLc65anqqyl1lsPsTbmCBF/3Kze5H75RCip9S8tXrleQw/Ndj0+JTzH4iroWOjePcvakYD+VXx56bPFT/EPvldLeL/pr9VrTCdEUS+vIK5Kl/W5u7dhb6OHE7O2/k5AEnkz35xQrQMVhqRY2DkWbBWAWq6qU/1GXYFyq4M5WDBqq0jB2cQ6LUMqt23HVuLJyBVAZpvJZPvqXI+X6/CuBnzwNrmvqOicR7X3oaa0DXeZ6NyN1c0qm+6kUJfcJXBgsUv6Tt83OiLNg9EE8FXuKLpuaTWfg0eUC0EddaZnz9uBICFZcoG/6oWo3W1RJXKZNlxmR3U01etxa6wP4PmQeVygGKNFo/6TJW3vs8aygctjqBHHFtZRCrWfOw0zspKyFzJ2MinkdkAPIVxCuKYx+QgyELe1zoWBwZr5LONz2CxXFnSVG+wtU1695jlfKLvn3pe2xpakDasDFYyB562PpdVNS7btsuyzXNfWbprKFvmCuapmfkCyF5elmxAkOLDyPNBfO7NMZu/4SPJX1Ku0qVzpw99YubSKeMbVJqRnT+JuSbeN/nzBDL/VzdCczOYmZ3DmGcJc4t5uL86hpnXZ5D4eR8uvj6CaUurkO9gDBPf/S/ofzaGGdsDhNVVm7qTQBZQ+dBHXPv7EbX0T0vFwt1sYZLN2UdEYGUal1IYtV5x1a83fU8iDbUd+VAj1QtNkdGgaUS3fHQvGF0prSZOFH41or483ZZb22qykcdyG5a8kb+6pN9JZdJon2UrjTH6vrAd2e0j32M9GVSKBgsj90vvNe4SKjQry/2wHJP9QKxed1LVvIN+d4VxHI280e/yKqzTcpyLy+h3JqhlCnclrLdZWJVJeUdBIX2FO1GqtkSp/dHvzCqkRR/b0WQTebHyMdYxc7tcb/V99AerjQdRY7bKrkTr5dNpbJd3Ush9Mw1G1QdK6p9lWTaqHwv97pka+WznMyjLVXmaerAg8nzdn9Qq55OeRZke9Xo11nyvEohWt4FlsKgwINbYn0Kg0PhzWUOjsm23LIugNibORoVljM+v+dxXPd3lqi1zJfLUagR75UWV3RZwi9b8wGCRCFBeH0PP+6PigKTUPBt2jCPxX9sw/fWh4vgP96MTmOgTwUabhp4/T8P/pMg+GbR4al9B6lZXsPB22vhFZUHeSlb6ocJGZLDUg3aU1iFbSXo+D6wsnkTa2m21bQzx/VkMf28eQ8en0POhs38AkezRb9/Ug5mN+tDStUteBaoKyxR43DD2TWFmx2n0l51PjTyxfncLrcX1V76K37+zxvE8LWxJcYvEjOLef41hqJkApbMPY9/3YdPSXGmAqoggR762islHhqBfuPydGiRwew9823wNHt02mh5rkd98K9JhCnL0Fpk3qwQoUjaOscPTYtkH0HVbHh//kQEK0fVNXIkeTRvf+7PGr/m+dokKdL8GT2/Y1A0iKyHZFZFBkgEKWRS+EK/67cz2tKwlRf/W1u4FjIroz26I4v3WGMYe74PHtYLUc30YKo478cJ75xKWPghi6nfD2HI6gMBzdruOyjXXkrJGj8eQeqwNM19R31RL1zS2pBDVZv0mVuNOmLVXQmR2g7fUVdGSIMX96BTi//BFLJ9M4/xnamYNLrcX7ls3o8PdDlebmrmSwmhfqaunaNs4Ei/1IFMIYPZEMGb5bZ0KqxnM/UgEDWrySgQpfS/OYaxLVGzfHC22wBAREdH6tCBICWHinyLQblGTa5A7PYbAM1W6Sg7Ekd5zsXRFu+FjUtZCwzjHoxAREbVciwfOtlYklkJ40wy0R5rvROl7NoZwdzs6utxwfbaCzLmLWH67H8NH1QKtov8IoR856w8qERER0bo4OEgx+uY63gmg/7DTOlE0jP16Cn2rCQwtaRjffh7TX+P4BSIiolba0K/FX4uw/KKx300h9Oh2bN2UwcJbzhzlkfskj5U2DcO3L2Di+wxQiIiIWs1xLSmh5xMIe4H85fNIHpU/UsihqERERDciR49JISIiohuX47p7iIiIiCQGKURERORIDFKIiIjIkRikEBERkSMxSCEiIiJHYpBCREREjsQghYiIiBwI+D/cX9+Lk4QQ+gAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "id": "c99cc661",
   "metadata": {},
   "source": [
    "In ridge regression, the regularization parameter (often denoted as \n",
    "�\n",
    "λ) controls the strength of regularization applied to the model. Ridge regression adds a penalty term to the least squares loss function, which is proportional to the squared magnitude of the coefficients. The objective of ridge regression is to minimize the sum of squared errors between the predicted and actual values, while also penalizing large coefficient values to prevent overfitting.\n",
    "\n",
    "The ridge regression cost function can be expressed as:\n",
    "\n",
    "![Screenshot%202024-02-17%20161715.png](attachment:Screenshot%202024-02-17%20161715.png)\n",
    "J(w)=MSE(w)+λ∑j=1 p w^2\n",
    "\n",
    "where: MSE(w) is the mean squared error term.\n",
    "\n",
    "λ is the regularization parameter.\n",
    "λ∑j=1 p w^2 is the sum of squared coefficients (excluding the intercept term).\n",
    "\n",
    "The regularization parameter λ controls the trade-off between fitting the training data well and keeping the model coefficients small. A larger value of λ results in stronger regularization, which penalizes larger coefficients more heavily. Conversely, a smaller value of λ results in weaker regularization, allowing the model to fit the training data more closely.\n",
    "\n",
    "The effect of the regularization parameter on the model's complexity can be understood as follows:\n",
    "\n",
    "High λ:\n",
    "\n",
    "Strong regularization.\n",
    "Leads to smaller coefficients.\n",
    "More shrinkage of coefficients towards zero.\n",
    "Simpler model with lower variance but potentially higher bias.\n",
    "Reduced risk of overfitting.\n",
    "\n",
    "Low λ:\n",
    "\n",
    "Weak regularization.\n",
    "Allows coefficients to take larger values.\n",
    "More flexibility in fitting the training data.\n",
    "Higher variance but potentially lower bias.\n",
    "Increased risk of overfitting, especially if the dataset is small or noisy.\n",
    "\n",
    "In summary, the regularization parameter in ridge regression controls the model's complexity by balancing the trade-off between fitting the training data well and preventing overfitting. Adjusting the regularization parameter allows for fine-tuning the model's performance based on the characteristics of the dataset and the desired balance between bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034b7e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adf16408",
   "metadata": {},
   "source": [
    "6.\n",
    "What are the advantages of using SVM (Support Vector Machine) over other classification algorithms?m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780dad1e",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVMs) offer several advantages over other classification algorithms, including:\n",
    "\n",
    "1.Effective in High-Dimensional Spaces:\n",
    "\n",
    "SVMs perform well even in high-dimensional spaces, making them suitable for datasets with many features, such as text classification or image recognition.\n",
    "\n",
    "2.Robust to Overfitting:\n",
    "\n",
    "SVMs are less prone to overfitting compared to other algorithms like decision trees, especially in high-dimensional spaces. This is because they maximize the margin between classes, which helps generalize better to unseen data.\n",
    "\n",
    "3.Works Well with Non-Linear Data:\n",
    "\n",
    "SVMs can efficiently handle non-linear decision boundaries using techniques like the kernel trick, which allows them to implicitly map the input data into higher-dimensional feature spaces.\n",
    "\n",
    "4.Global Optimization:\n",
    "\n",
    "SVMs solve a convex optimization problem, which means they find the globally optimal solution (given the constraints) rather than getting stuck in local minima, as can happen with neural networks.\n",
    "\n",
    "5.Effective in Small Sample Size:\n",
    "\n",
    "SVMs perform well even with a relatively small number of training examples. They are less affected by the curse of dimensionality compared to other algorithms like k-nearest neighbors.\n",
    "\n",
    "6.Versatile Kernel Functions:\n",
    "\n",
    "SVMs offer flexibility in choosing different kernel functions (e.g., linear, polynomial, radial basis function), allowing for customization based on the characteristics of the data and the problem at hand.\n",
    "\n",
    "7.Handles Imbalanced Data:\n",
    "\n",
    "SVMs can handle imbalanced datasets well, as they focus on maximizing the margin between classes rather than simply minimizing classification error.\n",
    "\n",
    "8.Memory Efficient:\n",
    "\n",
    "Once the SVM model is trained, only a subset of the training data points (support vectors) is retained in memory, making SVM memory efficient, especially when dealing with large datasets.\n",
    "\n",
    "9.Well-Studied and Documented:\n",
    "\n",
    "SVMs have been extensively studied and have well-documented theory and implementation libraries available in popular programming languages like Python (e.g., scikit-learn).\n",
    "\n",
    "Overall, SVMs offer a powerful and versatile approach to classification tasks, particularly in scenarios involving high-dimensional data, non-linear relationships, and the need for robust generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258091e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "340ba608",
   "metadata": {},
   "source": [
    "7.\n",
    "Discuss the concept of margin in the context of SVM (Support Vector Machine) and its significance in classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615d360c",
   "metadata": {},
   "source": [
    "In the context of Support Vector Machines (SVM), the margin refers to the separation between the decision boundary (hyperplane) and the nearest data points from each class, which are called support vectors. The margin is a key concept in SVM because SVM aims to find the hyperplane that maximizes this margin.\n",
    "\n",
    "The significance of the margin in SVM classification can be understood in several ways:\n",
    "\n",
    "1.Maximizing Margin:\n",
    "\n",
    "The primary objective of SVM is to find the hyperplane that maximizes the margin between classes. Maximizing the margin helps in achieving better generalization performance by providing a larger separation between classes, which reduces the risk of misclassification on unseen data.\n",
    "\n",
    "2.Robustness to Noise:\n",
    "\n",
    "A larger margin tends to be more robust to noise and outliers in the data. By maximizing the margin, SVMs can focus on the most informative data points (support vectors) while effectively ignoring noisy or irrelevant data points that lie farther away from the decision boundary.\n",
    "\n",
    "3.Control Overfitting:\n",
    "\n",
    "Maximizing the margin helps in controlling overfitting by finding a simpler decision boundary that generalizes well to unseen data. A wider margin corresponds to a simpler decision boundary, which is less likely to capture noise or small fluctuations in the data.\n",
    "\n",
    "4.Margin as a Measure of Confidence:\n",
    "\n",
    "The margin can also serve as a measure of confidence in the classification. Data points lying closer to the decision boundary are less confidently classified compared to those lying farther away. Therefore, maximizing the margin ensures more confident classification decisions.\n",
    "\n",
    "5.Support Vectors:\n",
    "\n",
    "The data points that lie on the margin or within the margin are called support vectors. These are the critical data points that define the margin and the decision boundary. By focusing on the support vectors, SVM effectively learns from the most informative data points, leading to a more robust and efficient model.\n",
    "\n",
    "In summary, the margin in SVM plays a crucial role in determining the optimal decision boundary by maximizing the separation between classes. It contributes to better generalization, robustness to noise, control overfitting, and provides a measure of confidence in classification decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d4892d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e01ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b7b512f",
   "metadata": {},
   "source": [
    "8.\n",
    "How does ridge regression address multicollinearity in the feature variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ff252",
   "metadata": {},
   "source": [
    "Ridge regression addresses multicollinearity in the feature variables by adding a penalty term to the ordinary least squares (OLS) loss function. Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, leading to instability in the estimation of coefficients and inflated standard errors.\n",
    "\n",
    "The penalty term added by ridge regression penalizes large coefficients, which tends to shrink them towards zero. This shrinkage effectively reduces the impact of multicollinearity by spreading the influence of correlated features across all features. Here's how ridge regression mitigates multicollinearity:\n",
    "\n",
    "1.Shrinking Coefficients:\n",
    "\n",
    "Ridge regression adds a penalty term to the OLS loss function, which is proportional to the sum of squared coefficients:\n",
    "\n",
    "J(w)=MSE(w)+λ∑j=1 p w^2\n",
    "\n",
    "The regularization parameter λ controls the strength of the penalty. As λ increases, the impact of the penalty term becomes stronger, leading to smaller coefficients.\n",
    "\n",
    "2.Redistribution of Influence:\n",
    "\n",
    "By shrinking the coefficients, ridge regression redistributes the influence of highly correlated features across all features. This helps in reducing the dominance of any single feature or subset of features, thus mitigating the effects of multicollinearity.\n",
    "\n",
    "3.Stability in Coefficient Estimates:\n",
    "\n",
    "Ridge regression provides more stable estimates of the coefficients, even in the presence of multicollinearity. The regularization term helps to stabilize the estimation process by reducing the variance of the coefficient estimates.\n",
    "\n",
    "4.Improved Generalization:\n",
    "\n",
    "By penalizing large coefficients, ridge regression leads to simpler models that generalize better to unseen data. This is particularly beneficial when dealing with multicollinearity, as it helps in building models that are less sensitive to small changes in the data.\n",
    "\n",
    "In summary, ridge regression addresses multicollinearity by shrinking the coefficients towards zero, redistributing the influence of correlated features, providing stable coefficient estimates, and improving the generalization performance of the model. It is a powerful technique for handling multicollinearity and improving the robustness of regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6822d016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63151970",
   "metadata": {},
   "source": [
    "9.\n",
    "Explain the working principle of the kernel trick in SVM and its significance in non-linear classification problems."
   ]
  },
  {
   "attachments": {
    "Screenshot%202024-02-17%20163125.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "f1d2e6e4",
   "metadata": {},
   "source": [
    "The kernel trick is a technique used in Support Vector Machines (SVMs) for solving non-linear classification problems. It allows SVMs to implicitly map the input data into higher-dimensional feature spaces without explicitly computing the transformed feature vectors. This enables SVMs to find non-linear decision boundaries in the original input space.\n",
    "\n",
    "Here's how the kernel trick works and its significance in non-linear classification problems:\n",
    "\n",
    "1.Linear Decision Boundaries:\n",
    "\n",
    "In its basic form, SVM seeks to find a linear decision boundary that separates the data points of different classes in the input space.\n",
    "\n",
    "2.Non-Linear Decision Boundaries:\n",
    "\n",
    "However, many real-world datasets are not linearly separable, meaning a single straight line cannot effectively separate the classes.\n",
    "\n",
    "3.Mapping to Higher-Dimensional Space:\n",
    "\n",
    "The kernel trick allows SVM to implicitly map the input data into a higher-dimensional feature space, where it may be easier to find a linear decision boundary that separates the classes.\n",
    "\n",
    "4.Kernel Functions:\n",
    "\n",
    "The key to the kernel trick is the use of kernel functions, which compute the dot product (similarity) between pairs of data points in the original input space without explicitly transforming the data into the higher-dimensional space.\n",
    "Common kernel functions include:\n",
    "![Screenshot%202024-02-17%20163125.png](attachment:Screenshot%202024-02-17%20163125.png)\n",
    "\n",
    "5.Decision Boundary in Feature Space:\n",
    "\n",
    "In the higher-dimensional feature space defined by the kernel function, SVM seeks to find a hyperplane that separates the classes. This hyperplane may appear non-linear when projected back into the original input space.\n",
    "\n",
    "6.Significance in Non-Linear Classification:\n",
    "\n",
    "-The significance of the kernel trick lies in its ability to handle non-linear classification problems by effectively transforming the data into a higher-dimensional space where classes may be more easily separable.\n",
    "\n",
    "-It allows SVM to capture complex decision boundaries without explicitly computing the feature vectors in the higher-dimensional space, thus avoiding the computational burden associated with explicit feature mapping.\n",
    "\n",
    "In summary, the kernel trick in SVM enables the algorithm to effectively address non-linear classification problems by implicitly mapping the data into higher-dimensional feature spaces and finding linear decision boundaries in those spaces. It offers a powerful and computationally efficient approach to handling non-linearities in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f481a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eedfd71f",
   "metadata": {},
   "source": [
    "10.\n",
    "What are the key assumptions of logistic regression, and how do they differ from those of linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011c7ed4",
   "metadata": {},
   "source": [
    "Logistic regression and linear regression are two different types of regression analysis used for different types of data and outcomes. Here are the key assumptions of logistic regression and how they differ from those of linear regression:\n",
    "\n",
    "Assumptions of Logistic Regression:\n",
    "\n",
    "1.Binary Outcome: Logistic regression assumes that the dependent variable (the outcome) is binary or dichotomous, meaning it takes on two possible outcomes (e.g., 0 or 1, yes or no).\n",
    "\n",
    "2.Independence of Observations: Logistic regression assumes that observations are independent of each other. In other words, the probability of one observation being in a certain category does not affect the probability of another observation being in that category.\n",
    "\n",
    "3.Linearity of Log-Odds: Logistic regression assumes that the relationship between the independent variables and the log-odds of the outcome is linear. However, this assumption pertains to the log-odds, not the original probabilities.\n",
    "\n",
    "4.No Multicollinearity: Logistic regression assumes that there is little to no multicollinearity among the independent variables. Multicollinearity occurs when independent variables are highly correlated with each other, which can affect the stability of the model estimates.\n",
    "\n",
    "5.Large Sample Size: While not strictly an assumption, logistic regression tends to perform better with a larger sample size, especially when estimating the coefficients of the independent variables.\n",
    "\n",
    "Differences from Linear Regression:\n",
    "\n",
    "1.Type of Outcome: The most fundamental difference is in the type of outcome variable each model is designed for. Linear regression is used when the dependent variable is continuous, while logistic regression is used when the dependent variable is binary.\n",
    "\n",
    "2.Distribution of Errors: Linear regression assumes that the errors (residuals) follow a normal distribution. In contrast, logistic regression assumes that the errors follow a binomial distribution, given the binary nature of the outcome variable.\n",
    "\n",
    "3.Nature of Relationship: Linear regression models the relationship between the independent variables and the mean of the dependent variable. In contrast, logistic regression models the relationship between the independent variables and the probability of the dependent variable being in a certain category.\n",
    "\n",
    "4.Scale of Predictors: In linear regression, predictors are assumed to be measured on an interval or ratio scale. In logistic regression, predictors can be of any scale, as the model only focuses on the log-odds of the outcome.\n",
    "\n",
    "In summary, while both logistic regression and linear regression are regression techniques, they differ in their assumptions, the type of outcome they are designed for, and the nature of the relationships they model between independent and dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a8f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f02c4f73",
   "metadata": {},
   "source": [
    "11.\n",
    "Can logistic regression be used for multi-class classification? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11084933",
   "metadata": {},
   "source": [
    "\n",
    "Yes, logistic regression can be extended to handle multi-class classification tasks. One common approach to achieve this is the \"one-vs-rest\" (OvR) or \"one-vs-all\" approach. Here's how it works:\n",
    "\n",
    "1.Training Phase:\n",
    "\n",
    "-For each class i, a separate logistic regression model is trained where the samples of that class are labeled as positive (1) and all other samples are labeled as negative (0).\n",
    "-Essentially, this creates k binary classifiers, where k is the number of classes in the dataset.\n",
    "\n",
    "2.Prediction Phase:\n",
    "\n",
    "-To classify a new data point, each binary classifier predicts the probability of the data point belonging to its corresponding class.\n",
    "-The final predicted class for the data point is the class with the highest predicted probability among all the binary classifiers.\n",
    "\n",
    "The significance of this approach lies in its simplicity and effectiveness. Each binary classifier is trained independently to distinguish between one class and the rest, simplifying the classification task. Additionally, logistic regression naturally provides probabilities for each class, allowing for easy interpretation of the model's confidence in its predictions.\n",
    "\n",
    "While the OvR approach is commonly used, there are other methods for multi-class classification with logistic regression, such as the \"softmax\" function, also known as the \"multinomial logistic regression\" or \"softmax regression.\" This approach directly models the probability distribution over all classes using a single logistic regression model and a softmax activation function, providing a more unified framework for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb6e1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8864f5e",
   "metadata": {},
   "source": [
    "12.\n",
    "How does the use of slack variables affect the margin and classification errors in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87371121",
   "metadata": {},
   "source": [
    "In Support Vector Machines (SVM), slack variables are introduced to allow for the classification of non-linearly separable datasets or datasets with some degree of overlap between classes. Slack variables relax the strict margin requirement imposed by hard-margin SVM, allowing for some misclassification of training points.\n",
    "\n",
    "Here's how the use of slack variables affects the margin and classification errors in SVM:\n",
    "\n",
    "1.Margin:\n",
    "\n",
    "-In a hard-margin SVM, the goal is to find the hyperplane that maximizes the margin between the two classes while ensuring that all training points are correctly classified.\n",
    "\n",
    "-Introducing slack variables allows for a soft-margin SVM, where some data points are allowed to be misclassified or fall within the margin or on the wrong side of the hyperplane.\n",
    "\n",
    "-The margin in a soft-margin SVM is still maximized, but it's a more flexible margin that can accommodate classification errors to achieve better generalization.\n",
    "\n",
    "2.Classification Errors:\n",
    "\n",
    "-Slack variables provide a mechanism for penalizing misclassifications in SVM. Each slack variable represents a measure of the distance from a data point to the correct side of the decision boundary.\n",
    "\n",
    "-The objective of soft-margin SVM is to minimize both the margin size and the total sum of slack variables, while still achieving a balance between maximizing the margin and minimizing classification errors.\n",
    "\n",
    "-The introduction of slack variables allows SVM to handle noisy or overlapping datasets by finding a compromise between maximizing the margin and tolerating some classification errors.\n",
    "\n",
    "In summary, the use of slack variables in SVM relaxes the strict margin requirement of hard-margin SVM, allowing for a soft-margin SVM that can handle non-linearly separable datasets and datasets with some degree of overlap between classes. Slack variables provide a mechanism for penalizing misclassifications while still maximizing the margin, resulting in a more robust and flexible classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13fa79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62f7966c",
   "metadata": {},
   "source": [
    "13.\n",
    "Compare and contrast the loss functions used in logistic regression and SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36144e47",
   "metadata": {},
   "source": [
    "While both logistic regression and Support Vector Machines (SVM) are used for classification tasks, they employ different loss functions to optimize model parameters. Here's a comparison of the loss functions used in logistic regression and SVM:\n",
    "\n",
    "Loss Function in Logistic Regression:\n",
    "\n",
    "1.Binary Cross-Entropy (Log Loss):\n",
    "-The loss function used in logistic regression is the binary cross-entropy loss, also known as log loss.\n",
    "\n",
    "-It is derived from the likelihood function and measures the difference between the predicted probabilities and the actual binary outcomes.\n",
    "\n",
    "-For a single sample, the binary cross-entropy loss is defined as:\n",
    "\n",
    "Loss= -[y log(hat(y))+ (1-y)log(1-hat(y))]\n",
    "where y is the actual label (0 or 1), and hat(y) is the predicted probability of the sample belonging to class 1.\n",
    "\n",
    "Loss Function in SVM:\n",
    "\n",
    "1.Hinge Loss:\n",
    "-The loss function used in SVM is the hinge loss.\n",
    "\n",
    "-The hinge loss is a margin-based loss function that penalizes misclassifications and encourages maximizing the margin between classes.\n",
    "\n",
    "-For a single sample, the hinge loss is defined as:\n",
    "\n",
    "Loss=max(0,1−y⋅f(x))\n",
    "where y is the actual label (-1 or 1), and f(x) is the decision function output (i.e., the signed distance from the hyperplane).\n",
    "\n",
    "Comparison:\n",
    "\n",
    "1.Nature:\n",
    "\n",
    "-Logistic regression loss is a probabilistic loss function that measures the likelihood of the predicted probabilities matching the actual outcomes.\n",
    "\n",
    "-SVM hinge loss is a geometric loss function that directly measures the margin between classes and penalizes misclassifications.\n",
    "\n",
    "2.Optimization:\n",
    "\n",
    "-Logistic regression minimizes the binary cross-entropy loss using optimization algorithms like gradient descent.\n",
    "\n",
    "-SVM minimizes the hinge loss using optimization algorithms like gradient descent or quadratic programming.\n",
    "\n",
    "3.Handling of Misclassifications:\n",
    "\n",
    "-Logistic regression penalizes misclassifications more severely, especially for samples far from the decision boundary, due to the logarithmic nature of the loss function.\n",
    "\n",
    "-SVM only penalizes misclassifications if they violate the margin, otherwise, the loss is zero. It focuses on maximizing the margin between classes.\n",
    "\n",
    "4.Probabilistic vs. Geometric:\n",
    "\n",
    "-Logistic regression provides probabilistic outputs (predicted probabilities) directly related to class membership probabilities.\n",
    "\n",
    "-SVM focuses on finding the hyperplane that maximizes the margin between classes, with no direct probabilistic interpretation.\n",
    "\n",
    "In summary, logistic regression and SVM use different loss functions tailored to their respective objectives and optimization approaches. Logistic regression's binary cross-entropy loss focuses on probabilistic predictions, while SVM's hinge loss emphasizes maximizing the margin between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f37caf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "311c4cbe",
   "metadata": {},
   "source": [
    "14.\n",
    "Explain the concept of L1 and L2 regularization in the context of logistic regression and their impact on model coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479df9f7",
   "metadata": {},
   "source": [
    "In the context of logistic regression, L1 and L2 regularization are techniques used to prevent overfitting by adding penalty terms to the cost function. These penalty terms discourage the model from learning complex relationships and help in controlling the size of the coefficients.\n",
    "\n",
    "1.L1 Regularization (Lasso):\n",
    "\n",
    "-In L1 regularization, the penalty term added to the cost function is the absolute value of the coefficients multiplied by a regularization parameter (λ).\n",
    "\n",
    "-The L1 regularization term is given by \n",
    "λ∑ j=1p ∣W(j)∣, where W(j)represents the coefficients of the model and λ controls the strength of regularization.\n",
    "\n",
    "-L1 regularization encourages sparsity in the model by driving some coefficients to exactly zero, effectively performing feature selection.\n",
    "\n",
    "-The impact of L1 regularization on the model coefficients is that it tends to produce sparse solutions with many coefficients being zero. This can lead to a simpler and more interpretable model, with only the most important features retained.\n",
    "\n",
    "2.L2 Regularization (Ridge):\n",
    "\n",
    "-In L2 regularization, the penalty term added to the cost function is the squared sum of the coefficients multiplied by a regularization parameter (λ).\n",
    "\n",
    "-The L2 regularization term is given by \n",
    "\n",
    "λ∑j=1p W(j)^2, where w(j)represents the coefficients of the model and λ controls the strength of regularization.\n",
    "\n",
    "-L2 regularization penalizes large coefficients but typically does not drive coefficients exactly to zero. Instead, it shrinks all coefficients towards zero proportionally.\n",
    "\n",
    "-The impact of L2 regularization on the model coefficients is that it tends to produce smaller and more balanced coefficients. This can help in reducing the effects of multicollinearity and stabilizing the model.\n",
    "\n",
    "Comparison:\n",
    "\n",
    "-L1 regularization (Lasso) tends to perform feature selection by driving some coefficients to zero, resulting in a sparse model.\n",
    "\n",
    "-L2 regularization (Ridge) penalizes large coefficients but typically does not force coefficients to zero, leading to smaller and more balanced coefficients.\n",
    "\n",
    "-The choice between L1 and L2 regularization depends on the specific characteristics of the dataset and the desired properties of the model. L1 regularization is preferred when feature selection is desired, while L2 regularization is preferred for stabilizing the model and reducing the impact of multicollinearity. Often, a combination of both (Elastic Net regularization) is used to benefit from the advantages of both types of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf486ead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
